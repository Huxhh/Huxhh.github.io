{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/Anisina/source/css/blog-style.css","path":"css/blog-style.css","modified":1,"renderable":1},{"_id":"themes/Anisina/source/css/syntax.styl","path":"css/syntax.styl","modified":1,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.eot","path":"fonts/glyphicons-halflings-regular.eot","modified":1,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.ttf","path":"fonts/glyphicons-halflings-regular.ttf","modified":1,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff","path":"fonts/glyphicons-halflings-regular.woff","modified":1,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff2","path":"fonts/glyphicons-halflings-regular.woff2","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/blog.js","path":"js/blog.js","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/bootstrap.min.js","path":"js/bootstrap.min.js","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.tagcloud.js","path":"js/jquery.tagcloud.js","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1},{"_id":"themes/Anisina/source/css/bootstrap.min.css","path":"css/bootstrap.min.css","modified":1,"renderable":1},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.svg","path":"fonts/glyphicons-halflings-regular.svg","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/Anisina/source/js/jquery.js","path":"js/jquery.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/Anisina/LICENSE","hash":"2b209f06bebeb2a8c2b7e187e436f3e1e1fbc8a7","modified":1536758339843},{"_id":"themes/Anisina/README.md","hash":"69a70e00c02bf563d425c8bcaf0b301a0eed5aa6","modified":1536758339844},{"_id":"themes/Anisina/_config.yml","hash":"5baf18c53f7635cd35aa5a8dab8abc08251023ce","modified":1536758339859},{"_id":"themes/Anisina/package.json","hash":"12541fbf56f785e4f5d486a55b4939f3033f625b","modified":1536758339864},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1539828394955},{"_id":"source/_posts/机器学习笔记（一）贝叶斯分类器.md","hash":"a47a2537347cb091b512d3a83558f920ba124b27","modified":1539828943942},{"_id":"themes/Anisina/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1536758339835},{"_id":"themes/Anisina/.git/config","hash":"050bd84e438fe3cde46d681fefd19a7d07cb0f15","modified":1536758339838},{"_id":"themes/Anisina/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1536758106311},{"_id":"themes/Anisina/.git/index","hash":"cd5206769a7088bd07316757ea0c33928e1601bb","modified":1536760023227},{"_id":"themes/Anisina/.git/packed-refs","hash":"850ed328dfe8c74f0ef8cb1bc70d5bede031b5f1","modified":1536758339832},{"_id":"themes/Anisina/.idea/Anisina.iml","hash":"4240dabdc746a36bae8a56eeec04a3a3fc59d842","modified":1536758339842},{"_id":"themes/Anisina/.idea/modules.xml","hash":"6f81355dee5b607683bdbd56595a930fb4b83c8b","modified":1536758339843},{"_id":"themes/Anisina/.idea/vcs.xml","hash":"c92f3eb0ad1c70371e177a4d7d741f90af3f902c","modified":1536758339843},{"_id":"themes/Anisina/.idea/workspace.xml","hash":"e0f2937282f1d9898ae485dc0c01b15bd7eaccfd","modified":1536758339843},{"_id":"themes/Anisina/Screenshots/mobile-index.jpeg","hash":"cd75f77f5d865d42182e2233e354eeba9f114d98","modified":1536758339845},{"_id":"themes/Anisina/languages_to_be_added/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1536758339859},{"_id":"themes/Anisina/languages_to_be_added/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1536758339859},{"_id":"themes/Anisina/languages_to_be_added/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1536758339859},{"_id":"themes/Anisina/languages_to_be_added/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1536758339859},{"_id":"themes/Anisina/languages_to_be_added/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1536758339859},{"_id":"themes/Anisina/languages_to_be_added/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1536758339860},{"_id":"themes/Anisina/languages_to_be_added/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1536758339860},{"_id":"themes/Anisina/languages_to_be_added/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1536758339860},{"_id":"themes/Anisina/languages_to_be_added/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1536758339860},{"_id":"themes/Anisina/layout/.DS_Store","hash":"fd623c7cbe0d8fd902f6ca242127fd8db7da6c18","modified":1536758339861},{"_id":"themes/Anisina/layout/404.ejs","hash":"1fe05722bd1b32bbe0ae4e3e880866f935e0ae11","modified":1536758339861},{"_id":"themes/Anisina/layout/index.ejs","hash":"a0eaee13571e79c3632e23a9e94ccc991761d1f5","modified":1536758339862},{"_id":"themes/Anisina/layout/layout.ejs","hash":"b728827bf3ec55baf96a882032397e6c74c65f34","modified":1536758339862},{"_id":"themes/Anisina/layout/page.ejs","hash":"e9990327469aa94a98a3dae92bdc9326a5b99c8a","modified":1536758339863},{"_id":"themes/Anisina/layout/poetry.ejs","hash":"6c955d419050825e13d39c780d45aceafbf6552d","modified":1536758339863},{"_id":"themes/Anisina/layout/post.ejs","hash":"05203c1f8414ffc237a00be77a156264df2c7971","modified":1536758339863},{"_id":"themes/Anisina/layout/tags.ejs","hash":"a5b73e70540e12532aa92f12609b3a937a8bc28b","modified":1536758339864},{"_id":"themes/Anisina/layout/works.ejs","hash":"1df954e54098cc4845295836374abed870789dcd","modified":1536758339864},{"_id":"themes/Anisina/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1536758106313},{"_id":"themes/Anisina/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1536758106311},{"_id":"themes/Anisina/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1536758106313},{"_id":"themes/Anisina/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1536758106314},{"_id":"themes/Anisina/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1536758106312},{"_id":"themes/Anisina/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1536758106314},{"_id":"themes/Anisina/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1536758106312},{"_id":"themes/Anisina/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1536758106313},{"_id":"themes/Anisina/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1536758106313},{"_id":"themes/Anisina/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1536758106315},{"_id":"themes/Anisina/.git/info/exclude","hash":"bb5a85730dcf100facee799c05cc4f6affec0745","modified":1536758106311},{"_id":"themes/Anisina/.git/logs/HEAD","hash":"56b0439dc6784cba85df0b0ca2bead5733c1a79d","modified":1536758339837},{"_id":"themes/Anisina/.idea/inspectionProfiles/Project_Default.xml","hash":"cb98213afbdfab7620cd4b6ba8801035079b4ae5","modified":1536758339842},{"_id":"themes/Anisina/Screenshots/Anisina.png","hash":"146dd991f55a827a514259e20a51de1e9b07a13d","modified":1536758339845},{"_id":"themes/Anisina/layout/_partial/footer.ejs","hash":"0235c9e7497e94fa036c3d21117834e4464a64e6","modified":1536758339861},{"_id":"themes/Anisina/layout/_partial/head.ejs","hash":"4e0d96cac503d4e3a5b254d8b8175c392971ce38","modified":1536758339861},{"_id":"themes/Anisina/layout/_partial/nav.ejs","hash":"3baa41d595e951efa1db34dd1789c6f8d3b094da","modified":1536758339862},{"_id":"themes/Anisina/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1536758339862},{"_id":"themes/Anisina/source/css/blog-style.css","hash":"c6830e31138e412c2aa05228c4cd6035063fe651","modified":1536758339865},{"_id":"themes/Anisina/source/css/syntax.styl","hash":"f3f9ff0d1ebc4f7fa18d7e367b2ba2f0899adbd4","modified":1536758339866},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1536758339866},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1536758339868},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1536758339868},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1536758339869},{"_id":"themes/Anisina/source/js/blog.js","hash":"0f805c744ef8a48c0abdd9d204cfc19ee6cafc14","modified":1536758339869},{"_id":"themes/Anisina/source/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1536758339869},{"_id":"themes/Anisina/source/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1536758339873},{"_id":"themes/Anisina/source/js/totop.js","hash":"11ede60fccb7c763d6973f80efc78b47c0843746","modified":1536758339873},{"_id":"themes/Anisina/Screenshots/poetry-show.png","hash":"f5fdcd25026a87a0aafeebb1f19cdb3c0a81a666","modified":1536758339858},{"_id":"themes/Anisina/source/css/bootstrap.min.css","hash":"c5db932e115ff97af7b4512b947cde3ba2964db8","modified":1536758339866},{"_id":"themes/Anisina/source/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1536758339868},{"_id":"themes/Anisina/source/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1536758339873},{"_id":"themes/Anisina/.git/objects/pack/pack-0a3cf5d407b7a0446aa4a2414e22d7347547fa50.idx","hash":"22da6e1633dc4768bfd268d388396c2c9c1ed8b6","modified":1536758339816},{"_id":"themes/Anisina/.git/refs/heads/master","hash":"248a07253021783cf98e361d4e7138d712330b61","modified":1536758339836},{"_id":"themes/Anisina/Screenshots/mobile-post.jpeg","hash":"2081cdff23a9a8c185a48d9aabcc9dc8e77833ec","modified":1536758339848},{"_id":"themes/Anisina/source/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1536758339872},{"_id":"themes/Anisina/.git/logs/refs/heads/master","hash":"56b0439dc6784cba85df0b0ca2bead5733c1a79d","modified":1536758339837},{"_id":"themes/Anisina/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1536758339835},{"_id":"themes/Anisina/Screenshots/pc-index.png","hash":"b04094dac75cb656b4244c1dfaf246168a0f8926","modified":1536758339851},{"_id":"themes/Anisina/.git/logs/refs/remotes/origin/HEAD","hash":"56b0439dc6784cba85df0b0ca2bead5733c1a79d","modified":1536758339835},{"_id":"themes/Anisina/Screenshots/pc-post.png","hash":"cde56c0797b6ff8dd555fb1f8c3f9b21bceaa3be","modified":1536758339857},{"_id":"themes/Anisina/.git/objects/pack/pack-0a3cf5d407b7a0446aa4a2414e22d7347547fa50.pack","hash":"31599716acea266f01b469b08d792dff9c5bab2b","modified":1536758339816},{"_id":"public/archives/index.html","hash":"d177a78eaaaf0a3f36cb8eba7af7d82b014223ee","modified":1539828957144},{"_id":"public/archives/2018/index.html","hash":"5d2a6644929008c9193e62d6317938cb66bf47cb","modified":1539828957145},{"_id":"public/archives/2018/10/index.html","hash":"16c4a75623e96837d203034d014f48be558843bc","modified":1539828957148},{"_id":"public/2018/10/18/机器学习笔记（一）贝叶斯分类器/index.html","hash":"9339c8cbbe8b7553e775fdf1d5de3d1c0132a590","modified":1539828957149},{"_id":"public/index.html","hash":"63395a5d99abb83a9c28304bc637a208e06a3e31","modified":1539828957151},{"_id":"public/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1539828957151},{"_id":"public/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1539828957151},{"_id":"public/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1539828957152},{"_id":"public/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1539828957152},{"_id":"public/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1539828957259},{"_id":"public/css/syntax.css","hash":"4616879fec214c9cc4f5835615348f0bbeabf2a9","modified":1539828957262},{"_id":"public/js/blog.js","hash":"0f805c744ef8a48c0abdd9d204cfc19ee6cafc14","modified":1539828957262},{"_id":"public/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1539828957262},{"_id":"public/js/totop.js","hash":"11ede60fccb7c763d6973f80efc78b47c0843746","modified":1539828957262},{"_id":"public/css/blog-style.css","hash":"c6830e31138e412c2aa05228c4cd6035063fe651","modified":1539828957262},{"_id":"public/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1539828957263},{"_id":"public/css/bootstrap.min.css","hash":"c5db932e115ff97af7b4512b947cde3ba2964db8","modified":1539828957263},{"_id":"public/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1539828957263},{"_id":"public/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1539828957263}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"机器学习笔记（一）贝叶斯分类器","date":"2018-10-18T01:53:16.000Z","header-img":"bayes.jpeg","_content":"\n# 机器学习笔记（一）贝叶斯分类器\n\n## 前言\n虽然前一两年就陆陆续续的了解了不少机器学习相关的知识，也在Coursera上学完了Andrew Ng的课程，但知识整体比较零碎，现在上了研究生开始上机器学习这一门课，算是比较系统的学习各个机器学习算法，因此做一些记录与总结。一方面起到课后复习汇总要点的作用，另一方面也便于日后用的时候能够立马找到参考。\n整体顺序将按照课程的顺序来，首先是与贝叶斯分类器相关的东西\n\n## 贝叶斯决策论\n贝叶斯决策论的基本思想可以归结为利用贝叶斯公式将已知的类条件概率密度与先验概率转换为后验概率，并依据后验概率的大小进行分类\n这里有必要复习一下先验概率与后验概率\n> **先验概率** 先验概率是指根据以往经验分析得到的结果，也可理解为统计概率，通常可以通过对数据中类别出现次数的统计得到类别的先验概率\n> **后验概率** 后验概率是指无法通过经验分析得到，需要根据先验概率与似然函数计算得到，可理解为某件事情已经发生，则引起该事件发生的各个因素的概率即为后验概率\n> **似然函数** 似然函数是统计学中的一个概念，是关于统计模型中参数的函数，具体来说，在给定输出x时，关于参数$\\theta$的似然函数$L(\\theta \\mid x)$等于给定参数$\\theta$后变量X的概率：$$L(\\theta\\mid x) = P(X=x\\mid\\theta)$$\n可以理解为我们要找到一个关于参数$\\theta$的函数来拟合已有的数据\n\n根据以上，贝叶斯公式可以表示为：\n$$posterior = {likelihood*prior \\over evidence}$$\n即\n$$P(c\\mid x) = {P(x\\mid c)P(c)\\over p(x)}$$\n其中$P(c\\mid x)$是我们希望求得的后验概率，$P(c)$是类的先验概率，$P(x\\mid c)$是类条件概率，通常可以通过似然函数使用极大似然估计计算出来。$P(x)$是用于归一化的证据因子，它对所有类别均相同，即与分类无关，可以当做一个已知的量。因此估计$P(c\\mid x)$的问题可以转化为利用现有数据训练估计先验概率$P(c)$与类别条件概率$P(c\\mid x)$\n\n为了衡量贝叶斯决策的分类效果，同时使用贝叶斯决策得到最好的分类效果，我们同时应定义错误率或者说条件风险。这里我们简化采用最小化分类错误率，我们可以将误判损失写为：\n* 若分类正确则$\\lambda = 0$\n* 若分类错误则$\\lambda = 1$\n\n此时条件风险为：\n$$R(c\\mid x) = 1 - \\lambda P(c\\mid x) = 1-P(c\\mid x)$$\n于是可以得到最小分类错误率的最优贝叶斯分类器为：\n$$h^*(x) = arg maxP(c\\mid x)$$\n即对每个样本x，选择能使后验概率$P(c\\mid x)$最大的类别标记\n根据前面，使后验概率最大可以转化为使类条件概率与先验概率的成绩最大，即：\n$$h^*(x) = arg maxP(c)P(x\\mid c)$$\n根据大数定律，当数据所包含的独立同分布的样本数量足够多时，$P(c)$可以通过计算频率近似得到。但类条件概率$P(x\\mid c)$无法通过频率计算得到，因为在现实中样本的取值可能是大于样本的数量的，即很多可能的取值在数据中没有出现，使用频率计算显然不可行。此时我们可以使用极大似然估计来对类条件概率进行估计。\n\n## 极大似然估计\n极大似然估计的思想即是使用上面所提到的释然函数的思想。首先假设所要估计的数据具有某种确定的概率分布形式，再根据数据对概率分布的参数进行估计，也便是找到一个概率分布对我们的数据进行拟合。具体地，我们想要估计$P(x\\mid c)$的概率分布，我们可以将其记为$P(x\\mid \\theta_c)$。接下来我们可以使用似然函数对参数$\\theta$进行估计。事实上，概率模型的训练过程就是参数估计的过程。\n在这里，似然函数可以写为：\n\n$$\\hat{\\theta}= arg maxLL(\\theta_c)$$\n具体如何取得最大值可以视似然函数的情况而定\n总结：极大似然函数就是试图在$\\theta_c$的所有可能中，中到一个能使数据出现的可能性最大的值\n\n## 朴素贝叶斯分类器\n在上面的式子中，我们需要求得$P(x\\mid c)$，但这里的$P(x\\mid c)$是在所有属性上的联合概率，即$x$表示的各个属性可能是有相互关联的，若要直接计算具有较大难度，而朴素贝叶斯分类器采取了条件独立性假设来避免复杂的计算，即假设各个属性之间是相互独立的。换句话说，即各个属性对于最终的分类独立地产生影响。“朴素”即指假设的**属性相互独立**这一条件，同时另外还有半朴素贝叶斯分类，此处不表。\n在此基础上，我们可以得到贝叶斯分类器的表示形式：\n$$h_{nb}(x) = arg maxP(c)\\prod_{i = 1}^dP(x_i\\mid c)$$\n朴素贝叶斯分类器的训练过程可以总结为使用给定的训练集D估计先验概率$P(c)$和类别条件概率$P(x_i\\mid c)$\n\n## 拉普拉斯平滑\n上面说到在计算类表条件概率$P(x\\mid c)$时需要使用极大似然估计去找到一个能够较好拟合数据的关于属性的概率分布，事实上，在计算离散属性时，我们可以使用频率来近似估计类别c中某属性出现的概率，并作为类别条件概率用于计算。例如，对语文本分类问题，我们可以统计某类文本中某个词的出现次数作为其条件概率用于计算。但使用这种方法时仍然会遇到我们上面提到的问题，即如果在数据中某个属性的某个取值没有出现，那么该取值的类别条件概率就为0，这样计算下来最终得到的后验概率也为0，这样显然是不合理的，我们可以使用拉普拉斯平滑来解决这一问题。\n我们使用$D_c$表示数据集D中分类为c的样本组成的集合，使用$D_{c, x_i}$表示$D_C$中第i个属性取值为$x_i$的样本组成的集合。不考虑平滑时计算公式为：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| \\over \\left|D_{c}\\right|} $$\n我们令$N_c$表示类别c的可能取值的数量，$N_i$表示属性$x_i$的可能取值的数量，采用拉普拉斯平滑修正后的式子为：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + 1 \\over \\left|D_{c}\\right| + N_c} $$\n同时，计算先验概率的式子也变换为：\n$$P(c) = {\\left|D_{c}\\right| + 1 \\over \\left|D \\right| + N_c}$$\n更一般地，可以使用参数$\\lambda$代替简单的加1，即：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + \\lambda \\over \\left|D_{c}\\right| + \\lambda N_c} $$\n同理：\n$$P(c) = {\\left|D_{c}\\right| + \\lambda \\over \\left|D \\right| + \\lambda N_c}$$\n采用这种方法可以便面由于数据不充分带来的概率估计值为0的问题。事实上，拉普拉斯平滑假设了类别与属性的均匀分布，算是额外引入的先验。\n\n\n","source":"_posts/机器学习笔记（一）贝叶斯分类器.md","raw":"---\ntitle: 机器学习笔记（一）贝叶斯分类器\ndate: 2018-10-18 09:53:16\nheader-img: bayes.jpeg\ntags:\n---\n\n# 机器学习笔记（一）贝叶斯分类器\n\n## 前言\n虽然前一两年就陆陆续续的了解了不少机器学习相关的知识，也在Coursera上学完了Andrew Ng的课程，但知识整体比较零碎，现在上了研究生开始上机器学习这一门课，算是比较系统的学习各个机器学习算法，因此做一些记录与总结。一方面起到课后复习汇总要点的作用，另一方面也便于日后用的时候能够立马找到参考。\n整体顺序将按照课程的顺序来，首先是与贝叶斯分类器相关的东西\n\n## 贝叶斯决策论\n贝叶斯决策论的基本思想可以归结为利用贝叶斯公式将已知的类条件概率密度与先验概率转换为后验概率，并依据后验概率的大小进行分类\n这里有必要复习一下先验概率与后验概率\n> **先验概率** 先验概率是指根据以往经验分析得到的结果，也可理解为统计概率，通常可以通过对数据中类别出现次数的统计得到类别的先验概率\n> **后验概率** 后验概率是指无法通过经验分析得到，需要根据先验概率与似然函数计算得到，可理解为某件事情已经发生，则引起该事件发生的各个因素的概率即为后验概率\n> **似然函数** 似然函数是统计学中的一个概念，是关于统计模型中参数的函数，具体来说，在给定输出x时，关于参数$\\theta$的似然函数$L(\\theta \\mid x)$等于给定参数$\\theta$后变量X的概率：$$L(\\theta\\mid x) = P(X=x\\mid\\theta)$$\n可以理解为我们要找到一个关于参数$\\theta$的函数来拟合已有的数据\n\n根据以上，贝叶斯公式可以表示为：\n$$posterior = {likelihood*prior \\over evidence}$$\n即\n$$P(c\\mid x) = {P(x\\mid c)P(c)\\over p(x)}$$\n其中$P(c\\mid x)$是我们希望求得的后验概率，$P(c)$是类的先验概率，$P(x\\mid c)$是类条件概率，通常可以通过似然函数使用极大似然估计计算出来。$P(x)$是用于归一化的证据因子，它对所有类别均相同，即与分类无关，可以当做一个已知的量。因此估计$P(c\\mid x)$的问题可以转化为利用现有数据训练估计先验概率$P(c)$与类别条件概率$P(c\\mid x)$\n\n为了衡量贝叶斯决策的分类效果，同时使用贝叶斯决策得到最好的分类效果，我们同时应定义错误率或者说条件风险。这里我们简化采用最小化分类错误率，我们可以将误判损失写为：\n* 若分类正确则$\\lambda = 0$\n* 若分类错误则$\\lambda = 1$\n\n此时条件风险为：\n$$R(c\\mid x) = 1 - \\lambda P(c\\mid x) = 1-P(c\\mid x)$$\n于是可以得到最小分类错误率的最优贝叶斯分类器为：\n$$h^*(x) = arg maxP(c\\mid x)$$\n即对每个样本x，选择能使后验概率$P(c\\mid x)$最大的类别标记\n根据前面，使后验概率最大可以转化为使类条件概率与先验概率的成绩最大，即：\n$$h^*(x) = arg maxP(c)P(x\\mid c)$$\n根据大数定律，当数据所包含的独立同分布的样本数量足够多时，$P(c)$可以通过计算频率近似得到。但类条件概率$P(x\\mid c)$无法通过频率计算得到，因为在现实中样本的取值可能是大于样本的数量的，即很多可能的取值在数据中没有出现，使用频率计算显然不可行。此时我们可以使用极大似然估计来对类条件概率进行估计。\n\n## 极大似然估计\n极大似然估计的思想即是使用上面所提到的释然函数的思想。首先假设所要估计的数据具有某种确定的概率分布形式，再根据数据对概率分布的参数进行估计，也便是找到一个概率分布对我们的数据进行拟合。具体地，我们想要估计$P(x\\mid c)$的概率分布，我们可以将其记为$P(x\\mid \\theta_c)$。接下来我们可以使用似然函数对参数$\\theta$进行估计。事实上，概率模型的训练过程就是参数估计的过程。\n在这里，似然函数可以写为：\n\n$$\\hat{\\theta}= arg maxLL(\\theta_c)$$\n具体如何取得最大值可以视似然函数的情况而定\n总结：极大似然函数就是试图在$\\theta_c$的所有可能中，中到一个能使数据出现的可能性最大的值\n\n## 朴素贝叶斯分类器\n在上面的式子中，我们需要求得$P(x\\mid c)$，但这里的$P(x\\mid c)$是在所有属性上的联合概率，即$x$表示的各个属性可能是有相互关联的，若要直接计算具有较大难度，而朴素贝叶斯分类器采取了条件独立性假设来避免复杂的计算，即假设各个属性之间是相互独立的。换句话说，即各个属性对于最终的分类独立地产生影响。“朴素”即指假设的**属性相互独立**这一条件，同时另外还有半朴素贝叶斯分类，此处不表。\n在此基础上，我们可以得到贝叶斯分类器的表示形式：\n$$h_{nb}(x) = arg maxP(c)\\prod_{i = 1}^dP(x_i\\mid c)$$\n朴素贝叶斯分类器的训练过程可以总结为使用给定的训练集D估计先验概率$P(c)$和类别条件概率$P(x_i\\mid c)$\n\n## 拉普拉斯平滑\n上面说到在计算类表条件概率$P(x\\mid c)$时需要使用极大似然估计去找到一个能够较好拟合数据的关于属性的概率分布，事实上，在计算离散属性时，我们可以使用频率来近似估计类别c中某属性出现的概率，并作为类别条件概率用于计算。例如，对语文本分类问题，我们可以统计某类文本中某个词的出现次数作为其条件概率用于计算。但使用这种方法时仍然会遇到我们上面提到的问题，即如果在数据中某个属性的某个取值没有出现，那么该取值的类别条件概率就为0，这样计算下来最终得到的后验概率也为0，这样显然是不合理的，我们可以使用拉普拉斯平滑来解决这一问题。\n我们使用$D_c$表示数据集D中分类为c的样本组成的集合，使用$D_{c, x_i}$表示$D_C$中第i个属性取值为$x_i$的样本组成的集合。不考虑平滑时计算公式为：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| \\over \\left|D_{c}\\right|} $$\n我们令$N_c$表示类别c的可能取值的数量，$N_i$表示属性$x_i$的可能取值的数量，采用拉普拉斯平滑修正后的式子为：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + 1 \\over \\left|D_{c}\\right| + N_c} $$\n同时，计算先验概率的式子也变换为：\n$$P(c) = {\\left|D_{c}\\right| + 1 \\over \\left|D \\right| + N_c}$$\n更一般地，可以使用参数$\\lambda$代替简单的加1，即：\n$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + \\lambda \\over \\left|D_{c}\\right| + \\lambda N_c} $$\n同理：\n$$P(c) = {\\left|D_{c}\\right| + \\lambda \\over \\left|D \\right| + \\lambda N_c}$$\n采用这种方法可以便面由于数据不充分带来的概率估计值为0的问题。事实上，拉普拉斯平滑假设了类别与属性的均匀分布，算是额外引入的先验。\n\n\n","slug":"机器学习笔记（一）贝叶斯分类器","published":1,"updated":"2018-10-18T02:15:43.942Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjndyejam0000gyodjnym5292","content":"<h1 id=\"机器学习笔记（一）贝叶斯分类器\"><a href=\"#机器学习笔记（一）贝叶斯分类器\" class=\"headerlink\" title=\"机器学习笔记（一）贝叶斯分类器\"></a>机器学习笔记（一）贝叶斯分类器</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>虽然前一两年就陆陆续续的了解了不少机器学习相关的知识，也在Coursera上学完了Andrew Ng的课程，但知识整体比较零碎，现在上了研究生开始上机器学习这一门课，算是比较系统的学习各个机器学习算法，因此做一些记录与总结。一方面起到课后复习汇总要点的作用，另一方面也便于日后用的时候能够立马找到参考。<br>整体顺序将按照课程的顺序来，首先是与贝叶斯分类器相关的东西</p>\n<h2 id=\"贝叶斯决策论\"><a href=\"#贝叶斯决策论\" class=\"headerlink\" title=\"贝叶斯决策论\"></a>贝叶斯决策论</h2><p>贝叶斯决策论的基本思想可以归结为利用贝叶斯公式将已知的类条件概率密度与先验概率转换为后验概率，并依据后验概率的大小进行分类<br>这里有必要复习一下先验概率与后验概率</p>\n<blockquote>\n<p><strong>先验概率</strong> 先验概率是指根据以往经验分析得到的结果，也可理解为统计概率，通常可以通过对数据中类别出现次数的统计得到类别的先验概率<br><strong>后验概率</strong> 后验概率是指无法通过经验分析得到，需要根据先验概率与似然函数计算得到，可理解为某件事情已经发生，则引起该事件发生的各个因素的概率即为后验概率<br><strong>似然函数</strong> 似然函数是统计学中的一个概念，是关于统计模型中参数的函数，具体来说，在给定输出x时，关于参数$\\theta$的似然函数$L(\\theta \\mid x)$等于给定参数$\\theta$后变量X的概率：$$L(\\theta\\mid x) = P(X=x\\mid\\theta)$$<br>可以理解为我们要找到一个关于参数$\\theta$的函数来拟合已有的数据</p>\n</blockquote>\n<p>根据以上，贝叶斯公式可以表示为：<br>$$posterior = {likelihood*prior \\over evidence}$$<br>即<br>$$P(c\\mid x) = {P(x\\mid c)P(c)\\over p(x)}$$<br>其中$P(c\\mid x)$是我们希望求得的后验概率，$P(c)$是类的先验概率，$P(x\\mid c)$是类条件概率，通常可以通过似然函数使用极大似然估计计算出来。$P(x)$是用于归一化的证据因子，它对所有类别均相同，即与分类无关，可以当做一个已知的量。因此估计$P(c\\mid x)$的问题可以转化为利用现有数据训练估计先验概率$P(c)$与类别条件概率$P(c\\mid x)$</p>\n<p>为了衡量贝叶斯决策的分类效果，同时使用贝叶斯决策得到最好的分类效果，我们同时应定义错误率或者说条件风险。这里我们简化采用最小化分类错误率，我们可以将误判损失写为：</p>\n<ul>\n<li>若分类正确则$\\lambda = 0$</li>\n<li>若分类错误则$\\lambda = 1$</li>\n</ul>\n<p>此时条件风险为：<br>$$R(c\\mid x) = 1 - \\lambda P(c\\mid x) = 1-P(c\\mid x)$$<br>于是可以得到最小分类错误率的最优贝叶斯分类器为：<br>$$h^<em>(x) = arg maxP(c\\mid x)$$<br>即对每个样本x，选择能使后验概率$P(c\\mid x)$最大的类别标记<br>根据前面，使后验概率最大可以转化为使类条件概率与先验概率的成绩最大，即：<br>$$h^</em>(x) = arg maxP(c)P(x\\mid c)$$<br>根据大数定律，当数据所包含的独立同分布的样本数量足够多时，$P(c)$可以通过计算频率近似得到。但类条件概率$P(x\\mid c)$无法通过频率计算得到，因为在现实中样本的取值可能是大于样本的数量的，即很多可能的取值在数据中没有出现，使用频率计算显然不可行。此时我们可以使用极大似然估计来对类条件概率进行估计。</p>\n<h2 id=\"极大似然估计\"><a href=\"#极大似然估计\" class=\"headerlink\" title=\"极大似然估计\"></a>极大似然估计</h2><p>极大似然估计的思想即是使用上面所提到的释然函数的思想。首先假设所要估计的数据具有某种确定的概率分布形式，再根据数据对概率分布的参数进行估计，也便是找到一个概率分布对我们的数据进行拟合。具体地，我们想要估计$P(x\\mid c)$的概率分布，我们可以将其记为$P(x\\mid \\theta_c)$。接下来我们可以使用似然函数对参数$\\theta$进行估计。事实上，概率模型的训练过程就是参数估计的过程。<br>在这里，似然函数可以写为：</p>\n<p>$$\\hat{\\theta}= arg maxLL(\\theta_c)$$<br>具体如何取得最大值可以视似然函数的情况而定<br>总结：极大似然函数就是试图在$\\theta_c$的所有可能中，中到一个能使数据出现的可能性最大的值</p>\n<h2 id=\"朴素贝叶斯分类器\"><a href=\"#朴素贝叶斯分类器\" class=\"headerlink\" title=\"朴素贝叶斯分类器\"></a>朴素贝叶斯分类器</h2><p>在上面的式子中，我们需要求得$P(x\\mid c)$，但这里的$P(x\\mid c)$是在所有属性上的联合概率，即$x$表示的各个属性可能是有相互关联的，若要直接计算具有较大难度，而朴素贝叶斯分类器采取了条件独立性假设来避免复杂的计算，即假设各个属性之间是相互独立的。换句话说，即各个属性对于最终的分类独立地产生影响。“朴素”即指假设的<strong>属性相互独立</strong>这一条件，同时另外还有半朴素贝叶斯分类，此处不表。<br>在此基础上，我们可以得到贝叶斯分类器的表示形式：<br>$$h_{nb}(x) = arg maxP(c)\\prod_{i = 1}^dP(x_i\\mid c)$$<br>朴素贝叶斯分类器的训练过程可以总结为使用给定的训练集D估计先验概率$P(c)$和类别条件概率$P(x_i\\mid c)$</p>\n<h2 id=\"拉普拉斯平滑\"><a href=\"#拉普拉斯平滑\" class=\"headerlink\" title=\"拉普拉斯平滑\"></a>拉普拉斯平滑</h2><p>上面说到在计算类表条件概率$P(x\\mid c)$时需要使用极大似然估计去找到一个能够较好拟合数据的关于属性的概率分布，事实上，在计算离散属性时，我们可以使用频率来近似估计类别c中某属性出现的概率，并作为类别条件概率用于计算。例如，对语文本分类问题，我们可以统计某类文本中某个词的出现次数作为其条件概率用于计算。但使用这种方法时仍然会遇到我们上面提到的问题，即如果在数据中某个属性的某个取值没有出现，那么该取值的类别条件概率就为0，这样计算下来最终得到的后验概率也为0，这样显然是不合理的，我们可以使用拉普拉斯平滑来解决这一问题。<br>我们使用$D_c$表示数据集D中分类为c的样本组成的集合，使用$D_{c, x_i}$表示$D_C$中第i个属性取值为$x_i$的样本组成的集合。不考虑平滑时计算公式为：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| \\over \\left|D_{c}\\right|} $$<br>我们令$N_c$表示类别c的可能取值的数量，$N_i$表示属性$x_i$的可能取值的数量，采用拉普拉斯平滑修正后的式子为：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + 1 \\over \\left|D_{c}\\right| + N_c} $$<br>同时，计算先验概率的式子也变换为：<br>$$P(c) = {\\left|D_{c}\\right| + 1 \\over \\left|D \\right| + N_c}$$<br>更一般地，可以使用参数$\\lambda$代替简单的加1，即：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + \\lambda \\over \\left|D_{c}\\right| + \\lambda N_c} $$<br>同理：<br>$$P(c) = {\\left|D_{c}\\right| + \\lambda \\over \\left|D \\right| + \\lambda N_c}$$<br>采用这种方法可以便面由于数据不充分带来的概率估计值为0的问题。事实上，拉普拉斯平滑假设了类别与属性的均匀分布，算是额外引入的先验。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"机器学习笔记（一）贝叶斯分类器\"><a href=\"#机器学习笔记（一）贝叶斯分类器\" class=\"headerlink\" title=\"机器学习笔记（一）贝叶斯分类器\"></a>机器学习笔记（一）贝叶斯分类器</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>虽然前一两年就陆陆续续的了解了不少机器学习相关的知识，也在Coursera上学完了Andrew Ng的课程，但知识整体比较零碎，现在上了研究生开始上机器学习这一门课，算是比较系统的学习各个机器学习算法，因此做一些记录与总结。一方面起到课后复习汇总要点的作用，另一方面也便于日后用的时候能够立马找到参考。<br>整体顺序将按照课程的顺序来，首先是与贝叶斯分类器相关的东西</p>\n<h2 id=\"贝叶斯决策论\"><a href=\"#贝叶斯决策论\" class=\"headerlink\" title=\"贝叶斯决策论\"></a>贝叶斯决策论</h2><p>贝叶斯决策论的基本思想可以归结为利用贝叶斯公式将已知的类条件概率密度与先验概率转换为后验概率，并依据后验概率的大小进行分类<br>这里有必要复习一下先验概率与后验概率</p>\n<blockquote>\n<p><strong>先验概率</strong> 先验概率是指根据以往经验分析得到的结果，也可理解为统计概率，通常可以通过对数据中类别出现次数的统计得到类别的先验概率<br><strong>后验概率</strong> 后验概率是指无法通过经验分析得到，需要根据先验概率与似然函数计算得到，可理解为某件事情已经发生，则引起该事件发生的各个因素的概率即为后验概率<br><strong>似然函数</strong> 似然函数是统计学中的一个概念，是关于统计模型中参数的函数，具体来说，在给定输出x时，关于参数$\\theta$的似然函数$L(\\theta \\mid x)$等于给定参数$\\theta$后变量X的概率：$$L(\\theta\\mid x) = P(X=x\\mid\\theta)$$<br>可以理解为我们要找到一个关于参数$\\theta$的函数来拟合已有的数据</p>\n</blockquote>\n<p>根据以上，贝叶斯公式可以表示为：<br>$$posterior = {likelihood*prior \\over evidence}$$<br>即<br>$$P(c\\mid x) = {P(x\\mid c)P(c)\\over p(x)}$$<br>其中$P(c\\mid x)$是我们希望求得的后验概率，$P(c)$是类的先验概率，$P(x\\mid c)$是类条件概率，通常可以通过似然函数使用极大似然估计计算出来。$P(x)$是用于归一化的证据因子，它对所有类别均相同，即与分类无关，可以当做一个已知的量。因此估计$P(c\\mid x)$的问题可以转化为利用现有数据训练估计先验概率$P(c)$与类别条件概率$P(c\\mid x)$</p>\n<p>为了衡量贝叶斯决策的分类效果，同时使用贝叶斯决策得到最好的分类效果，我们同时应定义错误率或者说条件风险。这里我们简化采用最小化分类错误率，我们可以将误判损失写为：</p>\n<ul>\n<li>若分类正确则$\\lambda = 0$</li>\n<li>若分类错误则$\\lambda = 1$</li>\n</ul>\n<p>此时条件风险为：<br>$$R(c\\mid x) = 1 - \\lambda P(c\\mid x) = 1-P(c\\mid x)$$<br>于是可以得到最小分类错误率的最优贝叶斯分类器为：<br>$$h^<em>(x) = arg maxP(c\\mid x)$$<br>即对每个样本x，选择能使后验概率$P(c\\mid x)$最大的类别标记<br>根据前面，使后验概率最大可以转化为使类条件概率与先验概率的成绩最大，即：<br>$$h^</em>(x) = arg maxP(c)P(x\\mid c)$$<br>根据大数定律，当数据所包含的独立同分布的样本数量足够多时，$P(c)$可以通过计算频率近似得到。但类条件概率$P(x\\mid c)$无法通过频率计算得到，因为在现实中样本的取值可能是大于样本的数量的，即很多可能的取值在数据中没有出现，使用频率计算显然不可行。此时我们可以使用极大似然估计来对类条件概率进行估计。</p>\n<h2 id=\"极大似然估计\"><a href=\"#极大似然估计\" class=\"headerlink\" title=\"极大似然估计\"></a>极大似然估计</h2><p>极大似然估计的思想即是使用上面所提到的释然函数的思想。首先假设所要估计的数据具有某种确定的概率分布形式，再根据数据对概率分布的参数进行估计，也便是找到一个概率分布对我们的数据进行拟合。具体地，我们想要估计$P(x\\mid c)$的概率分布，我们可以将其记为$P(x\\mid \\theta_c)$。接下来我们可以使用似然函数对参数$\\theta$进行估计。事实上，概率模型的训练过程就是参数估计的过程。<br>在这里，似然函数可以写为：</p>\n<p>$$\\hat{\\theta}= arg maxLL(\\theta_c)$$<br>具体如何取得最大值可以视似然函数的情况而定<br>总结：极大似然函数就是试图在$\\theta_c$的所有可能中，中到一个能使数据出现的可能性最大的值</p>\n<h2 id=\"朴素贝叶斯分类器\"><a href=\"#朴素贝叶斯分类器\" class=\"headerlink\" title=\"朴素贝叶斯分类器\"></a>朴素贝叶斯分类器</h2><p>在上面的式子中，我们需要求得$P(x\\mid c)$，但这里的$P(x\\mid c)$是在所有属性上的联合概率，即$x$表示的各个属性可能是有相互关联的，若要直接计算具有较大难度，而朴素贝叶斯分类器采取了条件独立性假设来避免复杂的计算，即假设各个属性之间是相互独立的。换句话说，即各个属性对于最终的分类独立地产生影响。“朴素”即指假设的<strong>属性相互独立</strong>这一条件，同时另外还有半朴素贝叶斯分类，此处不表。<br>在此基础上，我们可以得到贝叶斯分类器的表示形式：<br>$$h_{nb}(x) = arg maxP(c)\\prod_{i = 1}^dP(x_i\\mid c)$$<br>朴素贝叶斯分类器的训练过程可以总结为使用给定的训练集D估计先验概率$P(c)$和类别条件概率$P(x_i\\mid c)$</p>\n<h2 id=\"拉普拉斯平滑\"><a href=\"#拉普拉斯平滑\" class=\"headerlink\" title=\"拉普拉斯平滑\"></a>拉普拉斯平滑</h2><p>上面说到在计算类表条件概率$P(x\\mid c)$时需要使用极大似然估计去找到一个能够较好拟合数据的关于属性的概率分布，事实上，在计算离散属性时，我们可以使用频率来近似估计类别c中某属性出现的概率，并作为类别条件概率用于计算。例如，对语文本分类问题，我们可以统计某类文本中某个词的出现次数作为其条件概率用于计算。但使用这种方法时仍然会遇到我们上面提到的问题，即如果在数据中某个属性的某个取值没有出现，那么该取值的类别条件概率就为0，这样计算下来最终得到的后验概率也为0，这样显然是不合理的，我们可以使用拉普拉斯平滑来解决这一问题。<br>我们使用$D_c$表示数据集D中分类为c的样本组成的集合，使用$D_{c, x_i}$表示$D_C$中第i个属性取值为$x_i$的样本组成的集合。不考虑平滑时计算公式为：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| \\over \\left|D_{c}\\right|} $$<br>我们令$N_c$表示类别c的可能取值的数量，$N_i$表示属性$x_i$的可能取值的数量，采用拉普拉斯平滑修正后的式子为：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + 1 \\over \\left|D_{c}\\right| + N_c} $$<br>同时，计算先验概率的式子也变换为：<br>$$P(c) = {\\left|D_{c}\\right| + 1 \\over \\left|D \\right| + N_c}$$<br>更一般地，可以使用参数$\\lambda$代替简单的加1，即：<br>$$P(x_i\\mid c) = {\\left|D_{c, x_i}\\right| + \\lambda \\over \\left|D_{c}\\right| + \\lambda N_c} $$<br>同理：<br>$$P(c) = {\\left|D_{c}\\right| + \\lambda \\over \\left|D \\right| + \\lambda N_c}$$<br>采用这种方法可以便面由于数据不充分带来的概率估计值为0的问题。事实上，拉普拉斯平滑假设了类别与属性的均匀分布，算是额外引入的先验。</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}